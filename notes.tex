\documentclass[11pt]{article}

\usepackage{sectsty}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{epigraph} %quotes
\usepackage{bm} 
\usepackage{wrapfig} 
\usepackage{amssymb} %math symbols
\usepackage{mathtools} %more math stuff
\usepackage{amsthm} %theorems, proofs and lemmas
\usepackage[ruled,vlined]{algorithm2e} %algoritms/pseudocode

%% Theorem notation
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{problem}{Problem}[section]
\newtheorem{property}{Property}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]

%% declaring abs so that it works nicely
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

% Marges
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=5.5in
\textheight=9.0in
\headsep=0.5in


\title{Spectral Graph Matching and Quadratic Relaxations}
\date{\today}
\author{Noora Torpo \& Titouan Renard}

\begin{document}
\maketitle	

\section{The Graph Matching Problem}
\begin{wrapfigure}{r}{6cm}
    \includegraphics[width=6cm]{figures/Matching.pdf}
    \caption{A maximal graph matching for two graphs.}
\end{wrapfigure} 


The following section we define the graph matching problem and then we discuss the Gaussian-Wigner model of graph matching.

\subsection{General problem definition and initial remarks}

% \begin{center}
%     \includegraphics[width=0.4\textwidth]{figures/Matching.pdf}
% \end{center}



\begin{problem}
    Given two edge-weighted graphs $G_A,G_B$, s.t. $|V(G_A)| = |V(G_B)|$ and $|E(G_a)| = |E(G_b)|$, let $A$ and $B$ denote their respective weighted adjacency matrices. \textbf{Graph Matching} (or network alignment) refers to finding a bijection $\pi^*$ (which we can think of as a permutation $\pi\in \mathcal{S}_n$ on the $n$ labels of the vertices in the set $V(G_B)$) between the vertex sets $V_A,V_B$ of so that their edge sets $E_A,E_B$ are maximally aligned (with respect to their weights). The optimal solution $\pi^*$ of the graph matching problem satisfies:
      
    \begin{align}
        \max_{\pi^* \in \mathcal{S}_n} \sum^n_{i,j=1} A_{ij}B_{\pi(i)\pi(j)}.
        \label{QAP}
    \end{align}
\end{problem}

\begin{definition}
    (Adjacency Matrix) \textbf{Properties of the weighted adjacency matrix}, recall that given a graph $G = (V,E)$ and and edge-weighting function $w:E\rightarrow\mathbb{R}^+$ the adjacency matrix $A$ associated with $G,w$ is constructed as: 
    \[ (A_{ij} = w(e[i,j])). \]
\end{definition}

\begin{definition}
    (Spectral Decomposition) Given the adjacency matrix $A$ of the graph $G$ we write it's spectral decomposition as: 
    \begin{align*}
        (A_ij) = \left(\sum_{ij}^n \lambda_i u_i u_i^T\right), \\ 
        A = Q \Lambda Q^T.
    \end{align*}
\end{definition}

\subsection{The Gaussian Wigner Model}

\begin{wrapfigure}{r}{7cm}
    \includegraphics[width=7cm]{figures/gaussian_graph.png}
    \caption{$GOE(5)$ complete graph $G_a$ (left) and it's adjacency matrix $A$ (right).}
\end{wrapfigure} 


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.6\textwidth]{figures/gaussian_graph.png}
%     \caption{$GOE(5)$ complete graph $G_a$ (left) and it's adjacency matrix $A$ (right).}
% \end{figure}
% \textbf{Titou presents that, use the plots}
\begin{definition}
    (Gaussian Orthogonal Ensemble) we say that $A \in \mathbb{R}^{n\times n}$ is \textit{from the Gaussian Orthogonal Ensemble} ($A \sim GOE(n)$) if $A$ is symmetric and
    \begin{align*}
        (A_{ij}) = 
        \begin{cases}
            \sim \mathcal{N}(0,\frac{1}{n}) & i \neq j, \\
            \sim \mathcal{N}(0,\frac{2}{n}) & i = j.
        \end{cases}
    \end{align*}
\end{definition}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/GaussianWigner.png}
%     \caption{Generation of adjacency matrices with the Gaussian Wigner Model. From left to right $A$, $Z$, $A+\sigma Z$, $B$.}
% \end{figure}

\begin{definition}
    (Gaussian Wigner Model) We say that the graph pair $G_A,G_B$ described by the matrix pair $A,B \in \mathbb{R}^{n\times n}$ follows the Gaussian Wigner model if:
    \begin{align}
        B^{\pi^*} = A + \sigma Z,    
        \label{GaussianWigner}
    \end{align}
    where $A,Z \sim GOE(n)$ are independent and $\sigma \geq 0$ and $\pi^*$ is the permutation that we try to recover.
\end{definition}


% \newpage
\section{A spectral algorithm for the graph matching problem}

In the following section we use the following notation for the spectral decomposition of the adjacency matrices $A$ and $B$ of graphs $G_A$ and $G_B$ as:
\begin{align*}
    A = \sum^n_{i=1} \lambda_i u_i u_i^T, && B = \sum^n_{j=1} \mu_j v_j v_j^T,
\end{align*}
where the eigenvalues are sorted such that:
\begin{align*}
    \lambda_1 \geq ... \geq \lambda_n, && \mu_1 \geq ... \geq \mu_n.
\end{align*}



\begin{algorithm}
    \label{GRAMPA}
    \caption{Graph Matching by Pairwise eigen-Alignment (GRAMPA)}
    \textbf{Input:} Weighted adjacency matrices $A, B \in \mathbb{R}^{n\times n}$ and tuning parameter $\eta \in \mathbb{R}$Â \\
    \textbf{Output:} A permutation $\hat{\pi} \in \mathcal{S}_n$ \\
    Construct the similarity matrix:
    \begin{align}
        \hat{X} = \sum^n_{i,j} w(\lambda j,\mu_j) u_i^T u_i \bm{J} v_j v_j^T \in \mathbb{R}^{n \times n},
    \end{align}
    where $\bm{J}$ denotes the all ones $\mathbb{R}^{n \times n}$ matrix and $w$ is the Cauchy kernel of bandwidth $\eta$.
    \begin{align}
        w(x,y) = \frac{1}{(x-y)^2 + \eta^2}.
    \end{align}
    Output the permutation estimate $\hat{\pi}$ by rounding $\hat{X}$ to a permutation by solving the following linear assignment problem:
    \begin{align}
        \hat{\pi} = \underset{\pi \in \mathbb{S}_n}{argmax}\sum^n_{i=1} \hat{X}_{i, \pi(i)}.
    \end{align}
\end{algorithm}

\begin{property}
    \label{equivariance}
    (\textbf{equivariance}) let $\hat{\pi}(A,B)$ denote the output of Algorithm \ref{GRAMPA}, for any given permutation $\pi$ on $B$ (which gives the permuted matrix $B^\pi$ constructed as $B^\pi_{ij} = B_{\pi{i},\pi{j}}$) we have the following property:
    \begin{align}
        \pi \circ \hat{\pi} (A,B^\pi) = \hat{\pi} (A,B)
    \end{align}
\end{property}

\newpage
\subsection{Theorem 2.1 Outline}

The above described algorithm relays firmly on a theorem presented in the paper. At the last step of the algorithm, the similarity matrix is rounded to a vector that is claimed to present the estimation of the search permutation (add ref to line in algo). As we can see rounding is done by choosing the permutation that maximizes the sum of the diagonal elements after permuting the columns of the similarity matrix. The theorem, that ensures that this is indeed the best possible permutation, informally tells us that the diagonal elements of the similarity matrix are most likely higher than the other elements.

\begin{theorem}
 Consider the model (ref). There exist constants $c, c', > 0$ such that if 
 \begin{equation*}
     1/n^{0.1} \leq \eta \leq c/\log n \hspace{7mm} \text{and} \hspace{7mm} \sigma \leq c' \eta \text{,}
 \end{equation*}
 then with probability at least $1-n^{-4}$ for all large $n$, the matrix $\hat{X}$ in (ref) satisfies
 \begin{equation}
     \text{min}_{i \in [n]} \hat{X}_{i,\pi^*(i)} > \text{max}_{i,j \in [n]: j \neq \pi^*(i)} \hat{X}_{i,j}
 \end{equation}
 and hence, Algorithm \ref{GRAMPA} recovers $\hat{\pi}=\pi^*$.
\end{theorem}

To gain some intuition on why this proofs that the recovered permutation is the optimal one, let us verbalize what this result means. The theorem is claiming that most likely the "permuted diagonal" is always larger than the other elements in the matrix. This means that the similarity to the corresponding node between the two graphs is higher than the similarity between any other node. In noiseless setting this is obvious, the similarity to the node itself is clearly higher than or equal to the similarity with respect to other nodes. The theorem states that under some condition we can trust that most likely the same is observed even if there is noise. 

Let us next have a glance at the proof of the theorem. The property  \ref{equivariance} (equivariance) plays an important role in it. The property allows us to examine the identity permutation $\pi(i) = i$ because the corresponding result should be found with all permutations. Therefore we can examine two matrices: $X = \hat{X}(A,B)$ which is the similarity matrix between the graphs $A$ and $B$, and $X_* = \hat{X}(A,A)$ which presents the similarity matrix in a noiseless setting.

The writers present and proof two important lemmas that are used to complete the proof of the theorem. In short, the other lemmas states that under some conditions we gain lower bound for the diagonal elements of $X_*$ and upper bound to the other elements. The second lemma gives an upper bound for the maximum difference between the elements in $X_*$ and $X$. Note that both lemmas, work in a stochastic setting so the results hold with some probability. Large part of the paper is used to proof these two lemmas, but let us concentrate on the higher level induction. Combining these two lemmas and the equivariance property, the theorem is proved.


    

\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm]{figures/self_similarity.png}
    \caption{Illustration of the diagonal-dominance proof: self similarity matrix $\hat{X}$ of $A$ drawn from $GOE(200)$.}
\end{figure}



\newpage
\section{An optimization perspective on the problem}

The problem as stated in (\ref{QAP}) is an instance of the quadratic assignment problem a (QAP) which is NP-hard. Which can also be written in matrix notation as a combinatorial optimization problem on permutation matrices $\Pi\in \mathcal{G}_n$:
\begin{align*}
    \max_{\Pi\in \mathcal{G}_n}  \langle A,\Pi B \Pi^T \rangle \iff 
    \min_{\Pi\in \mathcal{G}_n} \| A - \Pi B \Pi^T \|_F^2 \iff
    \min_{\Pi\in \mathcal{G}_n} \| A \Pi - \Pi B \|_F^2 
\end{align*}
\begin{wrapfigure}{r}{7cm}
    \caption{A visual illustration of the quadratic unconstrained relaxation to the assignment problem.}\label{wrap-fig:1}
    \includegraphics[width=7cm]{figures/Projection.pdf}
    \label{fig:projection}
\end{wrapfigure} 

Which can in turn be relaxed from the set of permutations to it's convex hull (\textit{the Birkhoff polytope}) into the following a quadratic program which is convex with respect to the relaxed permutation matrix $X$:
\begin{align}
    \min_{X\in \mathcal{B}_n} \| A X - X B \|_F^2
    \label{RelaxedOPTIM}
\end{align}

Here the intuition is that a valid permutation matrix is some matrix filled with values $\Pi_{ij} \in \{1,0\}$, s.t all columns and all rows sum to $1$, we will find an approximation of the solution by finding an approximate solution matrix $X$ s.t. $X_{ij} \in [1,0]$ and all column and rows sum to $1$, which we will round to a permutation matrix. 



The set of all such matrices is given by the \textbf{Birkhoff polytope} which we formally define as: 
\begin{align*}
    \mathcal{B}_n := \{ X \in \mathbb{R}^{n \times n} : X\bm{1} = \bm{1},X^T\bm{1} = \bm{1}, X_{ij} \geq 0 ~ \forall i,j \},
\end{align*}
with $\bm{1}$ denoting the all ones $1 \times n$ vector.\\

We can show that the spectral method of Alg. \ref{GRAMPA}  outputs a matrix $\hat{X}$ which is a minimizer of the following quadratic problem:
\begin{align}
    \min_{X\in \mathbb{R}^{n\times n}} \| A X - X B \|_F^2 + \frac{\eta^2}{2} \|X\|^2_F - \bm{1}^T X \bm{1}. \label{SpectralOPTIM} 
\end{align}

Observe that the solution of the unconstrained $\hat{X}$ of (\ref{SpectralOPTIM}) is a scalar multiple of the solution $\tilde{X}$ of the constrained problem below :
\begin{align*}
    \min_{X\in \mathbb{R}^{n\times n}} \| A X - X B \|_F^2 + \frac{\eta^2}{2} \|X\|^2_F, \\
    \text{s.t.} ~ \bm{1}^T X \bm{1} = n.
\end{align*}
Where one can think of $\tilde{X}$ as a projection of $\hat{X}$ onto $\mathcal{B}$ (see figure \ref{fig:projection}). Differentiating the cost of (\ref{SpectralOPTIM}) by $X$ yield the gradient $A^2X+XB^2 - 2AXB + \eta^2X - \bm{J}$, which naturally gives gradient descent dynamics. 


\textbf{How do we show that?}\\
\textbf{Gradient descent on (\ref{SpectralOPTIM}).}\\

\textbf{Titou writes out the convergence proof}

\newpage
\section{Results}


\textbf{Noora}




\end{document}